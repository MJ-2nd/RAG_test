# RAG System with Advanced Tool-calling & MCP Support

ì´ í”„ë¡œì íŠ¸ëŠ” **ë‹¤ì–‘í•œ ìµœì‹  LLM ëª¨ë¸**ì„ ì§€ì›í•˜ëŠ” ê³ ê¸‰ RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œì…ë‹ˆë‹¤. **Tool-calling** ë° **MCP (Model Context Protocol)** ë¥¼ ì™„ì „ ì§€ì›í•˜ë©°, 32GB VRAM í™˜ê²½ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” íŠ¹ì§•

- **ë‹¤ì–‘í•œ LLM ì§€ì›**: Kimi K2, Qwen, DeepSeek, SmolLM3, Llama, Mistral ë“±
- **Tool-calling ì§€ì›**: ê³„ì‚°, ê²€ìƒ‰, íŒŒì¼ íƒìƒ‰ ë“± ë„êµ¬ ìë™ ì‹¤í–‰
- **MCP ì§€ì›**: Model Context Protocolë¡œ í™•ì¥ ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ êµ¬ì¡°
- **32GB VRAM ìµœì í™”**: ì–‘ìí™” ë° ë¶„ì‚° ì²˜ë¦¬ë¡œ íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©
- **ë“€ì–¼ GPU ì§€ì›**: Tensor Parallelismìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
- **ëª¨ë¸ë³„ ìµœì í™”**: ê° ëª¨ë¸ íƒ€ì…ì— ë§ëŠ” ì±„íŒ… í…œí”Œë¦¿ê³¼ ì„¤ì •

## ğŸ”§ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì§€ì› ëª¨ë¸ (2025ë…„ ìµœì‹ )
- **Kimi K2**: 1T MoE, 32B active, Native tool-calling (~16GB)
- **DeepSeek R1**: ì½”ë“œ íŠ¹í™”, Tool-calling ì§€ì› (7B~32B)
- **SmolLM3**: íš¨ìœ¨ì ì¸ ì†Œí˜• ëª¨ë¸, Tool-calling ì§€ì› (~6GB)
- **Qwen ì‹œë¦¬ì¦ˆ**: ë‹¤êµ­ì–´ ì§€ì›, ë¶€ë¶„ì  tool-calling (7B~32B)
- **Llama**: Metaì˜ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ (8B~70B)
- **Mistral/Mixtral**: MoE ì•„í‚¤í…ì²˜ (7B~8x7B)

### ë¦¬ì†ŒìŠ¤ í• ë‹¹
- **VRAM**: ëª¨ë¸ ì „ìš© (8GB~32GB, ëª¨ë¸ í¬ê¸°ì— ë”°ë¼)
- **CPU/RAM**: ì„ë² ë”© ëª¨ë¸ + ê²€ìƒ‰ ì‹œìŠ¤í…œ + ë„êµ¬ ì‹¤í–‰
- **Storage**: ëª¨ë¸ íŒŒì¼ (6GB~140GB), ì¸ë±ìŠ¤ íŒŒì¼

## âš¡ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •
```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ì¶”ê°€ íŒ¨í‚¤ì§€ (í•„ìš”ì‹œ)
pip install huggingface-hub transformers>=4.45.0
```

### 2. ëª¨ë¸ ì„ íƒ ë° ë‹¤ìš´ë¡œë“œ
```bash
# config.yamlì—ì„œ ì›í•˜ëŠ” ëª¨ë¸ ì„ íƒ
# ì˜ˆ: Kimi K2, DeepSeek R1, SmolLM3 ë“±

# ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
python download_models.py

# íŠ¹ì • ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ
python download_models.py --llm-only

# ë‹¤ìš´ë¡œë“œ ì „ ì •ë³´ í™•ì¸
python download_models.py --dry-run
```

### 3. ë¬¸ì„œ ì¤€ë¹„
```bash
# documents/ ë””ë ‰í† ë¦¬ì— ë¬¸ì„œ ë°°ì¹˜
cp your_documents.txt documents/
```

### 4. ì¸ë±ìŠ¤ êµ¬ì¶•
```bash
# ê¸°ë³¸ ì¸ë±ìŠ¤ ìƒì„±
python -m rag.build_index

# ì‚¬ìš©ì ì •ì˜ ì„¤ì •
python -m rag.build_index --doc_dir documents --index_path models/my_index
```

### 5. RAG ì‹œìŠ¤í…œ ì‹¤í–‰

#### ëŒ€í™”í˜• ëª¨ë“œ
```bash
python -m query.query_rag --interactive
```

#### FastAPI ì„œë²„ (Tool-calling ì§€ì›)
```bash
# LLM ì„œë²„ ì‹¤í–‰
python -m llm.app

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8000/docs ì ‘ì†
```

## ğŸ› ï¸ Tool-calling ì‚¬ìš©ë²•

### ê¸°ë³¸ ì œê³µ ë„êµ¬ë“¤

1. **ë¬¸ì„œ ê²€ìƒ‰** (`search_documents`)
   ```json
   {
     "name": "search_documents",
     "arguments": {
       "query": "ì¸ê³µì§€ëŠ¥ ê°œë°œ ë°©ë²•",
       "top_k": 5
     }
   }
   ```

2. **ìˆ˜í•™ ê³„ì‚°** (`calculate`)
   ```json
   {
     "name": "calculate", 
     "arguments": {
       "expression": "2 + 3 * 4"
     }
   }
   ```

3. **í˜„ì¬ ì‹œê°„** (`get_current_time`)
   ```json
   {
     "name": "get_current_time",
     "arguments": {}
   }
   ```

4. **íŒŒì¼ ê²€ìƒ‰** (`search_files`)
   ```json
   {
     "name": "search_files",
     "arguments": {
       "pattern": "*.py",
       "directory": "."
     }
   }
   ```

### ë„êµ¬ ì‚¬ìš© ì˜ˆì‹œ

**ì§ˆë¬¸**: "í˜„ì¬ ì‹œê°„ì„ ì•Œë ¤ì£¼ê³ , 2024ë…„ë¶€í„° ëª‡ ë…„ì´ ì§€ë‚¬ëŠ”ì§€ ê³„ì‚°í•´ì¤˜"

**LLM ìë™ ì‘ë‹µ** (ëª¨ë¸ì— ë”°ë¼ í˜•ì‹ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ):
```
<tool_call>
{"name": "get_current_time", "arguments": {}}
</tool_call>

<tool_call>
{"name": "calculate", "arguments": {"expression": "2025 - 2024"}}
</tool_call>

í˜„ì¬ ì‹œê°„ì€ 2025-01-XXì´ê³ , 2024ë…„ë¶€í„° 1ë…„ì´ ì§€ë‚¬ìŠµë‹ˆë‹¤.
```

## ğŸ”§ ìƒì„¸ ì„¤ì •

### ëª¨ë¸ ì„¤ì • (`llm/config.yaml`)

```yaml
llm:
  # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ (ì‰½ê²Œ ë³€ê²½ ê°€ëŠ¥)
  model_name: "moonshotai/Kimi-K2-Instruct"  # ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸
  model_path: "./models/kimi-k2-instruct"
  
  # Tool-calling ì„¤ì • (ëª¨ë¸ë³„ ìë™ ìµœì í™”)
  tool_calling:
    enabled: true
    format: "json"  # ëª¨ë¸ì— ë”°ë¼ ìë™ ì„ íƒ
    max_tools_per_call: 5
    parallel_tools: true
    
  # GPU ì„¤ì • (ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ìë™ ì¡°ì •)
  vllm:
    tensor_parallel_size: 2      # GPU ìˆ˜
    max_model_len: 32768         # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
    gpu_memory_utilization: 0.85
    
  # Generation ì„¤ì • (Tool-calling ìµœì í™”)
  generation:
    max_tokens: 2048
    temperature: 0.3  # Tool-callingì—ëŠ” ë‚®ì€ temperature ê¶Œì¥
    top_p: 0.9

# MCP ì„¤ì •
mcp:
  enabled: true
  protocol_version: "2025.1"
  tools_registry: "./tools/"

# RAG ì„¤ì • (Tool-calling ì—°ë™)
rag:
  chunk_size: 512
  chunk_overlap: 50
  top_k: 5
  similarity_threshold: 0.3
  enable_tool_retrieval: true
```

### ëª¨ë¸ë³„ ê¶Œì¥ ì„¤ì •

#### ê³ ì„±ëŠ¥ ëª¨ë¸ (32GB+ VRAM)
```yaml
llm:
  model_name: "moonshotai/Kimi-K2-Instruct"        # MoE, ~16GB
  # model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"  # ~65GB
```

#### ì¤‘ê°„ ì„±ëŠ¥ ëª¨ë¸ (16GB+ VRAM)
```yaml
llm:
  model_name: "Qwen/Qwen2.5-14B-Instruct-AWQ"     # ~6GB
  # model_name: "HuggingFaceTB/SmolLM3-3B-Instruct"     # ~6GB
```

#### íš¨ìœ¨ì  ëª¨ë¸ (8GB+ VRAM)
```yaml
llm:
  model_name: "Qwen/Qwen2.5-7B-Instruct"          # ~14GB
  # model_name: "mistralai/Mistral-7B-Instruct-v0.3"   # ~14GB
```

## ğŸŒ API ì‚¬ìš©ë²•

### FastAPI ì—”ë“œí¬ì¸íŠ¸

#### 1. í…ìŠ¤íŠ¸ ìƒì„± (Tool-calling í¬í•¨)
```bash
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "í˜„ì¬ ì‹œê°„ì„ ì•Œë ¤ì£¼ê³  2+3ì„ ê³„ì‚°í•´ì¤˜",
    "tools": [
      {
        "name": "get_current_time",
        "description": "Get current time",
        "parameters": {"type": "object", "properties": {}}
      },
      {
        "name": "calculate", 
        "description": "Perform calculations",
        "parameters": {
          "type": "object",
          "properties": {
            "expression": {"type": "string"}
          }
        }
      }
    ]
  }'
```

#### 2. ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡
```bash
curl "http://localhost:8000/tools"
```

#### 3. ìƒíƒœ í™•ì¸
```bash
curl "http://localhost:8000/health"
```

## ğŸ¯ ì„±ëŠ¥ ìµœì í™”

### VRAM ì ˆì•½ íŒ
1. **ëª¨ë¸ ì„ íƒ**: ìš©ë„ì— ë§ëŠ” ì ì ˆí•œ í¬ê¸° ì„ íƒ
2. **ì–‘ìí™” ì‚¬ìš©**: AWQ, BitsAndBytes 4bit ì–‘ìí™”
3. **ë“€ì–¼ GPU ë¶„ì‚°**: Tensor Parallelismìœ¼ë¡œ ë©”ëª¨ë¦¬ ë¶„ì‚°
4. **CPU ì„ë² ë”©**: VRAMì„ LLM ì „ìš©ìœ¼ë¡œ í™œìš©

### Tool-calling ì„±ëŠ¥ í–¥ìƒ
1. **ë‚®ì€ Temperature**: Tool-callingì—ëŠ” 0.3 ê¶Œì¥
2. **ë³‘ë ¬ ë„êµ¬ ì‹¤í–‰**: `parallel_tools: true`ë¡œ ì†ë„ í–¥ìƒ
3. **ëª¨ë¸ë³„ ìµœì í™”**: ê° ëª¨ë¸ì— ë§ëŠ” ì±„íŒ… í…œí”Œë¦¿ ìë™ ì„ íƒ

## ğŸ”§ ì‚¬ìš©ì ì •ì˜ ë„êµ¬ ì¶”ê°€

### 1. ë„êµ¬ ì •ì˜ íŒŒì¼ ìƒì„± (`tools/my_tool.json`)
```json
{
  "name": "my_custom_tool",
  "description": "ë‚´ ì‚¬ìš©ì ì •ì˜ ë„êµ¬",
  "parameters": {
    "type": "object",
    "properties": {
      "input": {"type": "string", "description": "ì…ë ¥ ê°’"}
    },
    "required": ["input"]
  }
}
```

### 2. ë„êµ¬ ì‹¤í–‰ ë¡œì§ ì¶”ê°€ (`llm/app.py`)
```python
async def execute_tool(self, tool_call: ToolCall) -> Dict[str, Any]:
    if tool_call.name == "my_custom_tool":
        input_value = tool_call.arguments.get('input')
        # ì‚¬ìš©ì ì •ì˜ ë¡œì§ êµ¬í˜„
        result = my_custom_logic(input_value)
        return {"result": result}
```

## ğŸ› ë¬¸ì œ í•´ê²°

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
```bash
# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¤„ì´ê¸°
# config.yamlì—ì„œ gpu_memory_utilizationì„ 0.8 ì´í•˜ë¡œ ì„¤ì •

# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
# SmolLM3-3B ë˜ëŠ” Qwen2.5-7Bë¡œ ë³€ê²½
```

### ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨
```bash
# ëª¨ë¸ íŒŒì¼ ì¬ë‹¤ìš´ë¡œë“œ
python download_models.py --llm-only

# HuggingFace í† í° ì„¤ì • (í•„ìš”ì‹œ)
huggingface-cli login
```

### Tool-calling ì˜¤ë¥˜
```bash
# ë„êµ¬ ë ˆì§€ìŠ¤íŠ¸ë¦¬ í™•ì¸
ls tools/
python -c "import json; print(json.load(open('tools/calculate.json')))"

# ëª¨ë¸ì´ tool-callingì„ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸
curl http://localhost:8000/health
```

## ğŸ’» ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´
- **GPU**: 8GB~32GB VRAM (ëª¨ë¸ì— ë”°ë¼)
  - **ì†Œí˜• ëª¨ë¸**: 8GB (SmolLM3, Qwen-7B)
  - **ì¤‘í˜• ëª¨ë¸**: 16GB (Qwen-14B, DeepSeek-14B)
  - **ëŒ€í˜• ëª¨ë¸**: 32GB (Kimi K2, DeepSeek-32B)
- **CPU**: ë©€í‹°ì½”ì–´ í”„ë¡œì„¸ì„œ (ì„ë² ë”© ì²˜ë¦¬ìš©)
- **RAM**: ìµœì†Œ 16GB, ê¶Œì¥ 32GB+
- **Storage**: ìµœì†Œ 100GB ì—¬ìœ  ê³µê°„

### ì†Œí”„íŠ¸ì›¨ì–´
- Python 3.9+
- CUDA 12.0+ (GPU ì‚¬ìš©ì‹œ)
- PyTorch 2.1+
- Transformers 4.45.0+

## ğŸš€ ì§€ì› ëª¨ë¸ ë¹„êµ

| ëª¨ë¸ | í¬ê¸° | VRAM | Tool-calling | íŠ¹ì§• |
|------|------|------|--------------|------|
| **Kimi K2** | 1T MoE (32B active) | ~16GB | âœ… Native | MoE, ìµœì‹  |
| **DeepSeek R1** | 7B~32B | ~14GB~65GB | âœ… Good | ì½”ë“œ íŠ¹í™” |
| **SmolLM3** | 3B | ~6GB | âœ… Good | íš¨ìœ¨ì  |
| **Qwen 2.5** | 7B~32B | ~14GB~65GB | âš ï¸ Partial | ë‹¤êµ­ì–´ |
| **Llama 3.1** | 8B~70B | ~16GB~140GB | âš ï¸ Limited | Meta |
| **Mistral** | 7B~8x7B | ~14GB~90GB | âš ï¸ Limited | MoE |

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

ë²„ê·¸ ë¦¬í¬íŠ¸, ê¸°ëŠ¥ ìš”ì²­, Pull Requestë¥¼ í™˜ì˜í•©ë‹ˆë‹¤.

## ğŸ’¬ ì§€ì›

ì§ˆë¬¸ì´ë‚˜ ë¬¸ì œê°€ ìˆìœ¼ì‹œë©´ ì´ìŠˆë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.