# LLM Model Configuration
# 16GB GPU x2 환경 SOTA 성능 설정 (Qwen3 MoE)
llm:
  # === 2025 SOTA 모델 (Qwen3 MoE) ===
  model_name: "Qwen/Qwen3-30B-A3B"  # MoE 30B total, 3B activated (~6GB)
  model_path: "./models/qwen3-30b-a3b"  # Local path (after download)
  
  # === 대안 SOTA 모델들 ===
  # 추론 특화 모델:
  # model_name: "Qwen/QwQ-32B-Preview"  # Chain-of-Thought 특화
  # model_path: "./models/qwq-32b-preview"
  
  # 안전한 고성능 모델들:
  # model_name: "Qwen/Qwen3-8B-Instruct"  # 8B, ~16GB, 매우 안전
  # model_name: "Qwen/Qwen3-14B-Instruct"  # 14B, ~28GB, 경계선
  
  # 기존 모델들 (보수적 선택):
  # model_name: "Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ"  # 32B AWQ (~6GB)
  # model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"  # 7B, ~14GB
  
  # VRAM optimization settings
  quantization:
    enabled: false  # MoE 모델은 이미 효율적
    
  # VLLM configuration (Qwen3 MoE 최적화)
  vllm:
    tensor_parallel_size: 2  # 2개 GPU 활용 (MoE 분산)
    max_model_len: 32768  # Qwen3 긴 컨텍스트 활용 (128K 지원)
    gpu_memory_utilization: 0.85  # 적당한 메모리 사용률
    enforce_eager: false  # MoE 성능 최적화
    
  # Generation parameters (Qwen3 최적화)
  generation:
    max_tokens: 2048
    temperature: 0.6  # Qwen3 권장 온도
    top_p: 0.9
    repetition_penalty: 1.1

# Embedding model configuration (CPU usage)
embedding:
  model_name: "BAAI/bge-m3"
  model_path: "./models/bge-m3"
  device: "cpu"  # Run on CPU to save VRAM
  batch_size: 32
  max_length: 1024
  
# RAG configuration
rag:
  chunk_size: 512
  chunk_overlap: 50
  top_k: 5
  similarity_threshold: 0.7
  
# Server configuration
server:
  host: "0.0.0.0"
  port: 8000
  workers: 1
