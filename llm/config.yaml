# LLM Model Configuration
# 32GB VRAM 최대 활용 설정 (성능 순 정렬)
llm:
  # === 현재 선택된 모델 (32GB VRAM 최대 활용) ===
  # model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"  # 14B (~28GB, 32GB 최적 활용)
  # model_path: "./models/deepseek-r1-14b"     # Local path (after download)
  
  # === 32GB VRAM 최대 활용 모델들 (성능 순) ===
  # 1순위: DeepSeek R1 14B (코드 특화, Tool-calling 지원, 32GB 최적)
  # model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B" # 14B (~28GB) 🥇 최고 성능
  
  # 2순위: Qwen2.5 32B AWQ (양자화로 32GB 활용, 다국어)
  # model_name: "Qwen/Qwen2.5-32B-Instruct-AWQ"           # 32B AWQ (~16GB) 🥈 고성능
  
  # 3순위: DeepSeek R1 32B AWQ (코드 특화, 양자화)
  model_name: "Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ" # 32B AWQ (~16GB) 🥉 코드 특화
  model_path: "./models/deepseek-r1-32b-awq"
  
  # 4순위: Qwen2.5 14B (다국어, 부분적 tool-calling)
  # model_name: "Qwen/Qwen2.5-14B-Instruct"               # 14B (~28GB) 🏅 다국어
  
  # 5순위: Llama 3.1 8B (안정적, 제한적 tool-calling)
  # model_name: "meta-llama/Llama-3.1-8B-Instruct"        # 8B (~16GB) 🏅 안정적
  
  # 6순위: Mistral 7B (MoE 아키텍처, 제한적 tool-calling)
  # model_name: "mistralai/Mistral-7B-Instruct-v0.3"      # 7B (~14GB) 🏅 MoE
  
  # 7순위: SmolLM3 3B (Tool-calling 특화, 효율적)
  # model_name: "HuggingFaceTB/SmolLM3-3B-Instruct"       # 3B (~6GB) 🏅 Tool-calling 특화
  
  # === 대용량 모델들 (32GB 초과, 양자화 필요) ===
  # Kimi K2 (실제 크기: 1T MoE ~65-90GB, 32GB VRAM 부족)
  # model_name: "moonshotai/Kimi-K2-Instruct"              # 1T MoE (~65-90GB) ❌ 32GB 부족
  # model_name: "moonshotai/Kimi-K2-Base"                  # 1T MoE (~65-90GB) ❌ 32GB 부족
  
  # DeepSeek R1 32B (양자화 없이는 32GB 부족)
  # model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B" # 32B (~65GB) ❌ 32GB 부족
  
  # Mixtral (MoE, 양자화 필요)
  # model_name: "mistralai/Mixtral-8x7B-Instruct-v0.1"    # MoE (~90GB) ❌ 32GB 부족
  
  # Tool-calling 설정 (모델별 자동 최적화)
  tool_calling:
    enabled: true
    format: "json"              # json 또는 xml (모델에 따라 자동 선택)
    max_tools_per_call: 5       # 한 번에 호출 가능한 도구 수
    tool_timeout: 30            # 도구 실행 타임아웃 (초)
    parallel_tools: true        # 병렬 도구 실행 지원
    
  # 양자화 설정 (모델 타입에 따라 자동 감지)
  quantization:
    enabled: false  # MoE/AWQ 모델은 이미 최적화됨
    method: "auto"  # auto, bitsandbytes, awq
    bits: 4         # 4 또는 8 (bitsandbytes용)
    
  # GPU 설정 (모델 크기에 따라 자동 조정)
  vllm:
    tensor_parallel_size: 2      # GPU 수에 따라 조정
    max_model_len: 32768         # 컨텍스트 길이 (모델별 최적화)
    gpu_memory_utilization: 0.85 # GPU 메모리 사용률
    enforce_eager: false         # MoE는 false, 일반 모델은 true
    enable_chunked_prefill: true # 효율적 prefill
    
  # Generation 파라미터 (Tool-calling 최적화)
  generation:
    max_tokens: 2048            # Tool-calling 응답용
    temperature: 0.3            # Tool-calling은 낮은 temperature 권장
    top_p: 0.9
    repetition_penalty: 1.05    # 반복 방지
    do_sample: true
    stop_sequences: ["<|im_end|>", "</tool_call>", "</function_call>"]

# Embedding model configuration (CPU 사용으로 VRAM 절약)
embedding:
  model_name: "BAAI/bge-m3"     # 다국어 지원 임베딩
  # model_name: "BAAI/bge-large-en-v1.5"           # 영어 특화
  # model_name: "sentence-transformers/all-MiniLM-L6-v2"  # 경량
  model_path: "./models/bge-m3"
  device: "cpu"  # CPU 사용으로 VRAM 절약
  batch_size: 32
  max_length: 1024
  
# RAG configuration (Tool-calling과 연동)
rag:
  chunk_size: 512
  chunk_overlap: 50
  top_k: 5
  similarity_threshold: 0.3
  enable_tool_retrieval: true   # Tool을 통한 검색 지원
  
# MCP (Model Context Protocol) 설정
mcp:
  enabled: true
  protocol_version: "2025.1"
  max_context_length: 32768
  tools_registry: "./tools/"    # MCP 도구 등록 디렉토리
  
# Server configuration
server:
  host: "0.0.0.0"
  port: 8000
  workers: 1
  enable_cors: true             # Tool-calling API 접근용
  max_request_size: "50MB"      # 대용량 tool 데이터 지원
