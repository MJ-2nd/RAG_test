# LLM Model Configuration
# 32GB VRAM 환경에서 안전한 설정 (AWQ 양자화 모델 우선)
llm:
  # === 메모리 안전한 모델 선택 (AWQ 양자화) ===
  # model_name: "Qwen/Qwen2.5-14B-Instruct-AWQ"  # 14B AWQ (~6GB)
  # model_path: "./models/qwen2.5-14b-instruct-awq"  # Local path (after download)
  
  # === 대안 모델들 (메모리 상황에 따라 선택) ===
  # 더 작은 AWQ 모델:
  # model_name: "Qwen/Qwen2.5-7B-Instruct-AWQ"   # 7B AWQ (~4GB)
  # model_name: "Qwen/Qwen2.5-3B-Instruct-AWQ"   # 3B AWQ (~2GB)
  
  # 더 큰 AWQ 모델 (메모리 여유시):
  model_name: "Qwen/Qwen2.5-32B-Instruct-AWQ"  # 32B AWQ (~16GB)
  model_path: "./models/qwen2.5-32b-instruct-awq"
  # model_name: "Qwen/Qwen2.5-72B-Instruct-AWQ"  # 72B AWQ (~36GB)
  
  # 일반 모델들 (메모리 여유시):
  # model_name: "Qwen/Qwen3-14B-Instruct"  # 14B 일반 모델 (~28GB)
  # model_name: "Qwen/Qwen3-8B-Instruct"   # 8B 일반 모델 (~16GB)
  
  # MoE 모델 (고성능 필요시):
  # model_name: "Qwen/Qwen3-30B-A3B"  # MoE 30B (~18.6GB)
  
  # 양자화 설정 (AWQ 모델 사용 시 자동 비활성화)
  quantization:
    enabled: false  # AWQ 모델은 이미 양자화됨
    method: "awq"   # 런타임 양자화 방식 (필요시)
    bits: 4         # 양자화 비트 수
    
  # VLLM configuration (메모리 최적화)
  vllm:
    tensor_parallel_size: 2  # 2개 GPU 활용
    max_model_len: 16384     # 컨텍스트 길이 (메모리 절약)
    gpu_memory_utilization: 0.8  # GPU 메모리 사용률
    enforce_eager: true      # 메모리 안정성 우선
    
  # Generation parameters
  generation:
    max_tokens: 1024
    temperature: 0.7
    top_p: 0.9
    repetition_penalty: 1.1

# Embedding model configuration (CPU usage)
embedding:
  model_name: "BAAI/bge-m3"
  model_path: "./models/bge-m3"
  device: "cpu"  # Run on CPU to save VRAM
  batch_size: 32
  max_length: 1024
  
# RAG configuration
rag:
  chunk_size: 512
  chunk_overlap: 50
  top_k: 5
  similarity_threshold: 0.3  # 테스트를 위해 낮춤
  
# Server configuration
server:
  host: "0.0.0.0"
  port: 8000
  workers: 1
