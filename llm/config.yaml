# LLM Model Configuration
llm:
  model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"  # SOTA reasoning model (32B params)
  # Alternative AWQ quantized: "Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ" (~6GB VRAM)
  model_path: "./models/deepseek-r1-distill-qwen-32b"  # Local path (after download)
  
  # VRAM optimization settings
  quantization:
    enabled: false  # AWQ model is pre-quantized, no runtime quantization needed
    bits: 4  # 4bit quantization for VRAM saving
    group_size: 128
    
  # VLLM configuration
  vllm:
    tensor_parallel_size: 2  # Use 2 GPUs for model parallelism
    max_model_len: 8192  # Context length
    gpu_memory_utilization: 0.9  # GPU memory utilization
    enforce_eager: false
    
  # Generation parameters
  generation:
    max_tokens: 2048
    temperature: 0.7
    top_p: 0.9
    repetition_penalty: 1.1

# Embedding model configuration (CPU usage)
embedding:
  model_name: "BAAI/bge-m3"
  model_path: "./models/bge-m3"
  device: "cpu"  # Run on CPU to save VRAM
  batch_size: 32
  max_length: 1024
  
# RAG configuration
rag:
  chunk_size: 512
  chunk_overlap: 50
  top_k: 5
  similarity_threshold: 0.7
  
# Server configuration
server:
  host: "0.0.0.0"
  port: 8000
  workers: 1
