# LLM Model Configuration
# 32GB VRAM ìµœëŒ€ í™œìš© ì„¤ì • (ì„±ëŠ¥ ìˆœ ì •ë ¬)
llm:
  # === í˜„ì¬ ì„ íƒëœ ëª¨ë¸ (32GB VRAM ìµœëŒ€ í™œìš©) ===
  # model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"  # 14B (~28GB, 32GB ìµœì  í™œìš©)
  # model_path: "./models/deepseek-r1-14b"     # Local path (after download)
  
  # === 32GB VRAM ìµœëŒ€ í™œìš© ëª¨ë¸ë“¤ (ì„±ëŠ¥ ìˆœ) ===
  # 1ìˆœìœ„: DeepSeek R1 14B (ì½”ë“œ íŠ¹í™”, Tool-calling ì§€ì›, 32GB ìµœì )
  # model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B" # 14B (~28GB) ğŸ¥‡ ìµœê³  ì„±ëŠ¥
  
  # 2ìˆœìœ„: Qwen2.5 32B AWQ (ì–‘ìí™”ë¡œ 32GB í™œìš©, ë‹¤êµ­ì–´)
  # model_name: "Qwen/Qwen2.5-32B-Instruct-AWQ"           # 32B AWQ (~16GB) ğŸ¥ˆ ê³ ì„±ëŠ¥
  
  # 3ìˆœìœ„: DeepSeek R1 32B AWQ (ì½”ë“œ íŠ¹í™”, ì–‘ìí™”)
  model_name: "Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ" # 32B AWQ (~16GB) ğŸ¥‰ ì½”ë“œ íŠ¹í™”
  model_path: "./models/deepseek-r1-32b-awq"
  
  # 4ìˆœìœ„: Qwen2.5 14B (ë‹¤êµ­ì–´, ë¶€ë¶„ì  tool-calling)
  # model_name: "Qwen/Qwen2.5-14B-Instruct"               # 14B (~28GB) ğŸ… ë‹¤êµ­ì–´
  
  # 5ìˆœìœ„: Llama 3.1 8B (ì•ˆì •ì , ì œí•œì  tool-calling)
  # model_name: "meta-llama/Llama-3.1-8B-Instruct"        # 8B (~16GB) ğŸ… ì•ˆì •ì 
  
  # 6ìˆœìœ„: Mistral 7B (MoE ì•„í‚¤í…ì²˜, ì œí•œì  tool-calling)
  # model_name: "mistralai/Mistral-7B-Instruct-v0.3"      # 7B (~14GB) ğŸ… MoE
  
  # 7ìˆœìœ„: SmolLM3 3B (Tool-calling íŠ¹í™”, íš¨ìœ¨ì )
  # model_name: "HuggingFaceTB/SmolLM3-3B-Instruct"       # 3B (~6GB) ğŸ… Tool-calling íŠ¹í™”
  
  # === ëŒ€ìš©ëŸ‰ ëª¨ë¸ë“¤ (32GB ì´ˆê³¼, ì–‘ìí™” í•„ìš”) ===
  # Kimi K2 (ì‹¤ì œ í¬ê¸°: 1T MoE ~65-90GB, 32GB VRAM ë¶€ì¡±)
  # model_name: "moonshotai/Kimi-K2-Instruct"              # 1T MoE (~65-90GB) âŒ 32GB ë¶€ì¡±
  # model_name: "moonshotai/Kimi-K2-Base"                  # 1T MoE (~65-90GB) âŒ 32GB ë¶€ì¡±
  
  # DeepSeek R1 32B (ì–‘ìí™” ì—†ì´ëŠ” 32GB ë¶€ì¡±)
  # model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B" # 32B (~65GB) âŒ 32GB ë¶€ì¡±
  
  # Mixtral (MoE, ì–‘ìí™” í•„ìš”)
  # model_name: "mistralai/Mixtral-8x7B-Instruct-v0.1"    # MoE (~90GB) âŒ 32GB ë¶€ì¡±
  
  # Tool-calling ì„¤ì • (ëª¨ë¸ë³„ ìë™ ìµœì í™”)
  tool_calling:
    enabled: true
    format: "json"              # json ë˜ëŠ” xml (ëª¨ë¸ì— ë”°ë¼ ìë™ ì„ íƒ)
    max_tools_per_call: 5       # í•œ ë²ˆì— í˜¸ì¶œ ê°€ëŠ¥í•œ ë„êµ¬ ìˆ˜
    tool_timeout: 30            # ë„êµ¬ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    parallel_tools: true        # ë³‘ë ¬ ë„êµ¬ ì‹¤í–‰ ì§€ì›
    
  # ì–‘ìí™” ì„¤ì • (ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ìë™ ê°ì§€)
  quantization:
    enabled: false  # MoE/AWQ ëª¨ë¸ì€ ì´ë¯¸ ìµœì í™”ë¨
    method: "auto"  # auto, bitsandbytes, awq
    bits: 4         # 4 ë˜ëŠ” 8 (bitsandbytesìš©)
    
  # GPU ì„¤ì • (ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ìë™ ì¡°ì •)
  vllm:
    tensor_parallel_size: 2      # GPU ìˆ˜ì— ë”°ë¼ ì¡°ì •
    max_model_len: 32768         # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (ëª¨ë¸ë³„ ìµœì í™”)
    gpu_memory_utilization: 0.85 # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
    enforce_eager: false         # MoEëŠ” false, ì¼ë°˜ ëª¨ë¸ì€ true
    enable_chunked_prefill: true # íš¨ìœ¨ì  prefill
    
  # Generation íŒŒë¼ë¯¸í„° (Tool-calling ìµœì í™”)
  generation:
    max_tokens: 2048            # Tool-calling ì‘ë‹µìš©
    temperature: 0.3            # Tool-callingì€ ë‚®ì€ temperature ê¶Œì¥
    top_p: 0.9
    repetition_penalty: 1.05    # ë°˜ë³µ ë°©ì§€
    do_sample: true
    stop_sequences: ["<|im_end|>", "</tool_call>", "</function_call>"]

# Embedding model configuration (CPU ì‚¬ìš©ìœ¼ë¡œ VRAM ì ˆì•½)
embedding:
  model_name: "BAAI/bge-m3"     # ë‹¤êµ­ì–´ ì§€ì› ì„ë² ë”©
  # model_name: "BAAI/bge-large-en-v1.5"           # ì˜ì–´ íŠ¹í™”
  # model_name: "sentence-transformers/all-MiniLM-L6-v2"  # ê²½ëŸ‰
  model_path: "./models/bge-m3"
  device: "cpu"  # CPU ì‚¬ìš©ìœ¼ë¡œ VRAM ì ˆì•½
  batch_size: 32
  max_length: 1024
  
# RAG configuration (Tool-callingê³¼ ì—°ë™)
rag:
  chunk_size: 512
  chunk_overlap: 50
  top_k: 5
  similarity_threshold: 0.3
  enable_tool_retrieval: true   # Toolì„ í†µí•œ ê²€ìƒ‰ ì§€ì›
  
# MCP (Model Context Protocol) ì„¤ì •
mcp:
  enabled: true
  protocol_version: "2025.1"
  max_context_length: 32768
  tools_registry: "./tools/"    # MCP ë„êµ¬ ë“±ë¡ ë””ë ‰í† ë¦¬
  
# Server configuration
server:
  host: "0.0.0.0"
  port: 8000
  workers: 1
  enable_cors: true             # Tool-calling API ì ‘ê·¼ìš©
  max_request_size: "50MB"      # ëŒ€ìš©ëŸ‰ tool ë°ì´í„° ì§€ì›
