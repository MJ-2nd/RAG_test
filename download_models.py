#!/usr/bin/env python3
"""
Model download script for RAG system
"""

import os
import argparse
from huggingface_hub import snapshot_download
import yaml

def load_config(config_path: str = "llm/config.yaml"):
    """Load configuration file"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def download_model(model_name: str, model_path: str):
    """Download model"""
    print(f"Downloading model: {model_name}")
    print(f"Save path: {model_path}")
    
    # Create directory
    os.makedirs(model_path, exist_ok=True)
    
    # Download model
    snapshot_download(
        repo_id=model_name,
        local_dir=model_path
    )
    
    print(f"Model download complete: {model_path}")

def estimate_model_size(model_name: str) -> str:
    """Estimate model size"""
    size_estimates = {
        # ğŸ¥‡ 1ìˆœìœ„: DeepSeek R1 14B (ìµœê³  ì„±ëŠ¥, 32GB ìµœì  í™œìš©)
        "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B": "~28GB (FP16), requires ~28GB VRAM ğŸ¥‡ ìµœê³  ì„±ëŠ¥, ì½”ë“œ íŠ¹í™”, Tool-calling ì§€ì›",
        
        # ğŸ¥ˆ 2ìˆœìœ„: Qwen2.5 32B AWQ (ê³ ì„±ëŠ¥, ì–‘ìí™”ë¡œ 32GB í™œìš©)
        "Qwen/Qwen2.5-32B-Instruct-AWQ": "~16GB (AWQ 4-bit), requires ~16GB VRAM ğŸ¥ˆ ê³ ì„±ëŠ¥, ë‹¤êµ­ì–´, ë¶€ë¶„ì  Tool-calling",
        
        # ğŸ¥‰ 3ìˆœìœ„: DeepSeek R1 32B AWQ (ì½”ë“œ íŠ¹í™”, ì–‘ìí™”)
        "Valdemardi/DeepSeek-R1-Distill-Qwen-32B-AWQ": "~16GB (AWQ 4-bit), requires ~16GB VRAM ğŸ¥‰ ì½”ë“œ íŠ¹í™”, Tool-calling ì§€ì›",
        
        # ğŸ… 4ìˆœìœ„: Qwen2.5 14B (ë‹¤êµ­ì–´, ë¶€ë¶„ì  tool-calling)
        "Qwen/Qwen2.5-14B-Instruct": "~28GB (FP16), requires ~28GB VRAM ğŸ… ë‹¤êµ­ì–´, ë¶€ë¶„ì  Tool-calling",
        
        # ğŸ… 5ìˆœìœ„: Llama 3.1 8B (ì•ˆì •ì , ì œí•œì  tool-calling)
        "meta-llama/Llama-3.1-8B-Instruct": "~16GB (FP16), requires ~16GB VRAM ğŸ… ì•ˆì •ì , ì œí•œì  Tool-calling",
        
        # ğŸ… 6ìˆœìœ„: Mistral 7B (MoE ì•„í‚¤í…ì²˜, ì œí•œì  tool-calling)
        "mistralai/Mistral-7B-Instruct-v0.3": "~14GB (FP16), requires ~14GB VRAM ğŸ… MoE, ì œí•œì  Tool-calling",
        
        # ğŸ… 7ìˆœìœ„: SmolLM3 3B (Tool-calling íŠ¹í™”, íš¨ìœ¨ì )
        "HuggingFaceTB/SmolLM3-3B-Instruct": "~6GB (FP16), requires ~6GB VRAM ğŸ… Tool-calling íŠ¹í™”, íš¨ìœ¨ì ",
        
        # ê¸°íƒ€ ëª¨ë¸ë“¤
        "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B": "~14GB (FP16), requires ~14GB VRAM âœ… Tool-calling ì§€ì›",
        "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B": "~65GB (FP16), requires 64GB+ VRAM âŒ 32GB ë¶€ì¡±",
        "Qwen/Qwen2.5-7B-Instruct": "~14GB (FP16), requires ~14GB VRAM âš ï¸ ë¶€ë¶„ì  tool-calling",
        "Qwen/Qwen2.5-14B-Instruct-AWQ": "~6GB (AWQ 4-bit), requires ~6GB VRAM âš ï¸ ë¶€ë¶„ì  tool-calling",
        "meta-llama/Llama-3.1-70B-Instruct": "~140GB (FP16), requires ë¶„ì‚° ì²˜ë¦¬ âŒ 32GB ë¶€ì¡±",
        "mistralai/Mixtral-8x7B-Instruct-v0.1": "~90GB (MoE), requires 64GB+ VRAM âŒ 32GB ë¶€ì¡±",
        "moonshotai/Kimi-K2-Instruct": "~65-90GB (1T MoE), requires 64GB+ VRAM âŒ 32GB ë¶€ì¡±",
        "moonshotai/Kimi-K2-Base": "~65-90GB (1T MoE), requires 64GB+ VRAM âŒ 32GB ë¶€ì¡±",
        "Valdemardi/DeepSeek-R1-Distill-Qwen-14B-AWQ": "~8GB (AWQ 4-bit), requires ~8GB VRAM âœ… 32GBì— ì•ˆì „",
        "Valdemardi/DeepSeek-R1-Distill-Qwen-7B-AWQ": "~4GB (AWQ 4-bit), requires ~4GB VRAM âœ… ë§¤ìš° ì•ˆì „",
        
        # Embedding models
        "BAAI/bge-m3": "~2.3GB",
        "BAAI/bge-large-en-v1.5": "~1.3GB",
        "sentence-transformers/all-MiniLM-L6-v2": "~90MB"
    }
    
    return size_estimates.get(model_name, "Size information not available")

def main():
    parser = argparse.ArgumentParser(description="Download models for RAG system")
    parser.add_argument("--config", default="llm/config.yaml", help="Configuration file path")
    parser.add_argument("--llm-only", action="store_true", help="Download LLM model only")
    parser.add_argument("--embedding-only", action="store_true", help="Download embedding model only")
    parser.add_argument("--dry-run", action="store_true", help="Show information only without actual download")
    
    args = parser.parse_args()
    
    # Load configuration
    config = load_config(args.config)
    
    llm_config = config['llm']
    embedding_config = config['embedding']
    
    print("=== RAG System Model Download ===")
    print(f"Configuration file: {args.config}")
    print()
    
    # Print model information
    print("Models to download:")
    if not args.embedding_only:
        llm_size = estimate_model_size(llm_config['model_name'])
        print(f"  LLM: {llm_config['model_name']}")
        print(f"       Path: {llm_config['model_path']}")
        print(f"       Estimated size: {llm_size}")
    
    if not args.llm_only:
        embedding_size = estimate_model_size(embedding_config['model_name'])
        print(f"  Embedding: {embedding_config['model_name']}")
        print(f"            Path: {embedding_config['model_path']}")
        print(f"            Estimated size: {embedding_size}")
    
    print()
    
    # VRAM usage prediction
    print("=== Memory Usage Prediction ===")
    model_name_lower = llm_config['model_name'].lower()
    quantization_config = llm_config.get('quantization', {})
    
    # Model type detection and memory prediction (ì„±ëŠ¥ ìˆœ)
    if "deepseek" in model_name_lower and "14b" in model_name_lower and "awq" not in model_name_lower:
        print("ğŸ¥‡ LLM (DeepSeek R1-14B): ~28GB total, ~14GB per GPU with tensor_parallel_size=2")
        print("âœ… 32GB VRAM ìµœì  í™œìš© (87.5% ì‚¬ìš©)")
        print("ğŸ”§ ì½”ë“œ íŠ¹í™”, Tool-calling ì§€ì›")
        print("ğŸ¯ 32GB í™˜ê²½ì—ì„œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸")
    elif "qwen" in model_name_lower and "32b" in model_name_lower and "awq" in model_name_lower:
        print("ğŸ¥ˆ LLM (Qwen2.5-32B-AWQ): ~16GB total, ~8GB per GPU with tensor_parallel_size=2")
        print("âœ… 32GB VRAM íš¨ìœ¨ì  í™œìš© (50% ì‚¬ìš©)")
        print("ğŸŒ ë‹¤êµ­ì–´ ì§€ì›, ë¶€ë¶„ì  Tool-calling")
        print("ğŸ¯ ì–‘ìí™”ë¡œ 32B ì„±ëŠ¥ì„ 16GBë¡œ ì••ì¶•")
    elif "deepseek" in model_name_lower and "32b" in model_name_lower and "awq" in model_name_lower:
        print("ğŸ¥‰ LLM (DeepSeek R1-32B-AWQ): ~16GB total, ~8GB per GPU with tensor_parallel_size=2")
        print("âœ… 32GB VRAM íš¨ìœ¨ì  í™œìš© (50% ì‚¬ìš©)")
        print("ğŸ”§ ì½”ë“œ íŠ¹í™”, Tool-calling ì§€ì›")
        print("ğŸ¯ 32B ì„±ëŠ¥ì„ ì–‘ìí™”ë¡œ 16GBë¡œ ì••ì¶•")
    elif "qwen" in model_name_lower and "14b" in model_name_lower and "awq" not in model_name_lower:
        print("ğŸ… LLM (Qwen2.5-14B): ~28GB total, ~14GB per GPU with tensor_parallel_size=2")
        print("âœ… 32GB VRAM ìµœì  í™œìš© (87.5% ì‚¬ìš©)")
        print("ğŸŒ ë‹¤êµ­ì–´ ì§€ì›, ë¶€ë¶„ì  Tool-calling")
        print("ğŸ¯ ë‹¤êµ­ì–´ ì„±ëŠ¥ì— íŠ¹í™”")
    elif "llama" in model_name_lower and "8b" in model_name_lower:
        print("ğŸ… LLM (Llama-3.1-8B): ~16GB total, ~8GB per GPU with tensor_parallel_size=2")
        print("âœ… 32GB VRAM íš¨ìœ¨ì  í™œìš© (50% ì‚¬ìš©)")
        print("ğŸ¦™ Meta's ì•ˆì •ì  ëª¨ë¸, ì œí•œì  Tool-calling")
        print("ğŸ¯ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•")
    elif "mistral" in model_name_lower and "7b" in model_name_lower and "mixtral" not in model_name_lower:
        print("ğŸ… LLM (Mistral-7B): ~14GB total, ~7GB per GPU with tensor_parallel_size=2")
        print("âœ… 32GB VRAM íš¨ìœ¨ì  í™œìš© (43.75% ì‚¬ìš©)")
        print("ğŸ”§ MoE ì•„í‚¤í…ì²˜, ì œí•œì  Tool-calling")
        print("ğŸ¯ MoEì˜ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥")
    elif "smollm3" in model_name_lower:
        print("ğŸ… LLM (SmolLM3-3B): ~6GB total, single GPU ê°€ëŠ¥")
        print("âœ… 32GB VRAM ì—¬ìœ ë¡œìš´ í™œìš© (18.75% ì‚¬ìš©)")
        print("ğŸ”§ Tool-calling íŠ¹í™” íŒŒì¸íŠœë‹")
        print("ğŸ¯ Tool-callingì— ìµœì í™”ëœ íš¨ìœ¨ì  ëª¨ë¸")
    elif "deepseek" in model_name_lower:
        if "32b" in model_name_lower:
            print("LLM (DeepSeek R1-32B): ~65GB total, ~32GB per GPU")
            print("âŒ 32GB VRAM í™˜ê²½ì—ëŠ” ë¶€ì¡±")
        elif "7b" in model_name_lower:
            print("LLM (DeepSeek R1-7B): ~14GB total, single GPU ê°€ëŠ¥")
            print("âœ… 32GB VRAM í™˜ê²½ì— ì•ˆì „")
        print("ğŸ”§ Code-specialized with tool-calling")
    elif "qwen" in model_name_lower:
        if "32b" in model_name_lower:
            if "awq" in model_name_lower:
                print("LLM (Qwen2.5-32B-AWQ): ~16GB total, ~8GB per GPU")
                print("âœ… 32GB VRAM í™˜ê²½ì— ì í•© (AWQ ì–‘ìí™”)")
            else:
                print("LLM (Qwen2.5-32B): ~65GB total, ~32GB per GPU")
                print("âŒ 32GB VRAM í™˜ê²½ì—ëŠ” ë¶€ì¡±")
        elif "14b" in model_name_lower:
            if "awq" in model_name_lower:
                print("LLM (Qwen2.5-14B-AWQ): ~6GB total, single GPU ê°€ëŠ¥")
                print("âœ… 32GB VRAM í™˜ê²½ì— ìµœì  (AWQ ì–‘ìí™”)")
            else:
                print("LLM (Qwen2.5-14B): ~28GB total, ~14GB per GPU")
                print("âœ… 32GB VRAM í™˜ê²½ì— ì í•©")
        elif "7b" in model_name_lower:
            print("LLM (Qwen2.5-7B): ~14GB total, single GPU ê°€ëŠ¥")
            print("âœ… 32GB VRAM í™˜ê²½ì— ì•ˆì „")
        print("ğŸŒ Multilingual support, âš ï¸ ë¶€ë¶„ì  tool-calling")
    elif "kimi" in model_name_lower or "moonshotai" in model_name_lower:
        print("LLM (Kimi K2): ~65-90GB total (1T MoE)")
        print("âŒ 32GB VRAM í™˜ê²½ì—ëŠ” ë¶€ì¡± (64GB+ í•„ìš”)")
        print("ğŸ’¡ ëŒ€ì•ˆ: DeepSeek R1-14B ì‚¬ìš© ê¶Œì¥")
    elif "llama" in model_name_lower:
        if "70b" in model_name_lower:
            print("LLM (Llama-70B): ~140GB total, ë¶„ì‚° ì²˜ë¦¬ í•„ìˆ˜")
            print("âŒ 32GB í™˜ê²½ì—ì„œëŠ” ì–‘ìí™” ì—†ì´ ë¶ˆê°€ëŠ¥")
        print("ğŸ¦™ Meta's open-source LLM, âš ï¸ ì œí•œì  tool-calling")
    elif "mistral" in model_name_lower or "mixtral" in model_name_lower:
        if "mixtral" in model_name_lower:
            print("LLM (Mixtral MoE): ~90GB total, ~45GB per GPU")
            print("âŒ 32GB í™˜ê²½ì—ì„œëŠ” ì–‘ìí™” í•„ìš”")
            print("ğŸ”§ MoE architecture")
        else:
            print("LLM (Mistral-7B): ~14GB total, single GPU ê°€ëŠ¥")
            print("âœ… 32GB VRAM í™˜ê²½ì— ì•ˆì „")
        print("âš ï¸ ì œí•œì  tool-calling")
    elif "awq" in model_name_lower:
        if "32b" in model_name_lower:
            print("LLM (32B-AWQ): ~16GB total, ~8GB per GPU with tensor_parallel_size=2")
            print("âœ… 32GB VRAM í™˜ê²½ì— ì í•© (AWQ ì–‘ìí™”)")
        elif "14b" in model_name_lower:
            print("LLM (14B-AWQ): ~6-8GB total, single GPU ê°€ëŠ¥")
            print("âœ… 32GB VRAM í™˜ê²½ì— ì í•© (AWQ ì–‘ìí™”)")
        elif "7b" in model_name_lower:
            print("LLM (7B-AWQ): ~3-4GB total, single GPU ê°€ëŠ¥")
            print("âœ… ë§¤ìš° ì—¬ìœ ìˆëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©")
    elif "32b" in model_name_lower:
        if quantization_config.get('enabled', False):
            method = quantization_config.get('method', 'unknown')
            bits = quantization_config.get('bits', 16)
            
            if method == 'bitsandbytes' and bits == 4:
                print("LLM (32B + 4bit BitsAndBytes): ~28GB total, ~14GB per GPU")
                print("âœ… 32GB VRAM í™˜ê²½ì— ì í•© (ì–‘ìí™”)")
            elif method == 'fp8' or bits == 8:
                print("LLM (32B + 8bit FP8): ~32GB total, ~16GB per GPU")
                print("âš ï¸  32GB VRAM í™˜ê²½ì—ì„œ í•œê³„")
            else:
                print("LLM (32B + quantization): ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶ˆëª…í™•")
        else:
            print("LLM (32B): ~65GB total, ~32GB per GPU")
            print("âŒ 32GB VRAM í™˜ê²½ì—ëŠ” ë¶€ì¡±")
    elif "14b" in model_name_lower:
        print("LLM (14B): ~28GB total, ~14GB per GPU with tensor_parallel_size=2")
        print("âœ… 32GB VRAM í™˜ê²½ì— ì í•©")
    elif "7b" in model_name_lower:
        print("LLM (7B): ~14GB total, single GPU ê°€ëŠ¥")
        print("âœ… 32GB VRAM í™˜ê²½ì— ì•ˆì „")
    elif "3b" in model_name_lower:
        print("LLM (3B): ~6GB total, single GPU ê°€ëŠ¥")
        print("âœ… 32GB VRAM í™˜ê²½ì— ë§¤ìš° ì•ˆì „")
    elif "1.5b" in model_name_lower:
        print("LLM (1.5B): ~3GB total")
        print("âœ… CPU ì‹¤í–‰ë„ ê°€ëŠ¥")
    
    print("Embedding model: CPU usage (RAM ~2-4GB)")
    print("FAISS index: CPU usage (RAM ~hundreds of MB)")
    print()
    
    if args.dry_run:
        print("Dry run mode: No actual download will be performed.")
        return
    
    # User confirmation
    response = input("Do you want to start the download? (y/N): ").lower().strip()
    if response != 'y':
        print("Download cancelled.")
        return
    
    try:
        # Download LLM model
        if not args.embedding_only:
            download_model(llm_config['model_name'], llm_config['model_path'])
        
        # Download embedding model
        if not args.llm_only:
            download_model(embedding_config['model_name'], embedding_config['model_path'])
        
        print("\n=== All Model Downloads Complete ===")
        print("Next steps:")
        print("1. Place documents in documents/ directory")
        print("2. Build index: python -m rag.build_index")
        print("3. Run RAG system: python -m query.query_rag --interactive")
        
    except Exception as e:
        print(f"Download error: {e}")
        print("Please check your network connection and try again.")

if __name__ == "__main__":
    main() 